# -*- coding: utf-8 -*-
"""IITM_KA1_23f3001900

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/drishya23f3001900/iitm-ka1-23f3001900.b69acbdf-7d4d-4503-a484-0419b1ce5542.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250905/auto/storage/goog4_request%26X-Goog-Date%3D20250905T091537Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db30e7de435c40b952acf4b756828b7dc18f5af174e446f2cbae147a5e1c81419408576a7e171ad7b6517efc4d99b5d2ba24887c1a3976a2e48fac21daf314b0a53ecfe34858adfbef4a5deb217492bf8ef777d9d51cd72d33e90195808b19fa7de544ea4324cfd53937378f21428ec694f5f605b20bb0c504498db07216de534006ab6ef8bfed9b5494a592999020fbd2da175639eaa44bb4b81dd396e6a999bc1a1db2ca8582dfbf770fa3f4dbb13ff965bcc2d0043df4cb4f47b4a6a8941e48e467226d6a424b28231aa2871cc912f0f7b6c1b70e6ebf34d9136cfd321a09ea1ed92357d11a34610804ef0ab4617cedd4dbb03888fd9caca62155c55e2c786
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

mlp_term_2_2025_kaggle_assignment_1_path = kagglehub.competition_download('mlp-term-2-2025-kaggle-assignment-1')

print('Data source import complete.')

"""# ðŸ“Œ Problem Statement
### The objective of this assignment is to build a regression model that accurately predicts the airline ticket prices based on various features such as the airline name, source and destination cities, departure and arrival times, stops, travel class, duration, and number of days left for departure.

### Importing different libraries used for data processing
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns #plotting
import matplotlib.pyplot as plt


import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""#### Reading train.csv and test.csv"""

train=pd.read_csv("/kaggle/input/mlp-term-2-2025-kaggle-assignment-1/train.csv")
test=pd.read_csv("/kaggle/input/mlp-term-2-2025-kaggle-assignment-1/test.csv")

"""# Identifying Data Types"""

dtypes=pd.DataFrame({
    'Column Name': train.columns,
    'Data types': train.dtypes.values
})
dtypes

#other methods
#train.dtypes <--- not a method but a property
#train.info()  <--- method/function

"""# Descriptive Statistics of Data"""

train.describe()

"""# Handling Missing values

### Identifying Missing values
"""

missing=train.isnull().sum() #missing is pandas Series (1D labelled array)
#print(missing)
missing=missing[missing>0] #filter outs columns which do not have missing values
missing_df=pd.DataFrame({
    "Column_Name" : missing.index,
    "Number of missing values" : missing.values
})
#print(missing)
missing_df

"""### Handling these values

* For **CATEGORICAL** data: Use **MODE** imputation
* For NUMERICAL data: Use **MEAN** or **MEDIAN** imputation

#### Categorical Columns (airplane,departure,stops)
"""

categorical=["airline","departure","stops"]

train_clean=train.copy()
for col in categorical:
    mode_value = train[col].mode()[0]
    train_clean[col] = train[col].fillna(mode_value)

"""#### Numerical Columns (duration, days_left)

These are used to check the skewness of the graph and how the data is distributed.
If the Graph is skewed,it is better to use MEDIAN imputation(or using median values to fill missing values)
If the Graph is evenly distributed , it is better to use MEAN imputation
"""

#sns.histplot(train['duration'].dropna(), kde=True)
#sns.histplot(train['days_left'].dropna(), kde=True)
print(train['days_left'].skew())
print(train['duration'].skew())

"""
**> So it is better to use MEAN imputation for "days_left" And MEDIAN imputation for "duration"**
"""

numerical=["duration","days_left"]

for col in numerical:
    if abs(train[col].skew())>0.5:
        train_clean[col] = train[col].fillna(train[col].median()) #more robust for skewed data
    else:
        train_clean[col] = train[col].fillna(train[col].mean()) #better for symmeteric distribution

train_clean.isnull().sum() #to show that now no missing values are there

"""# Handling Duplicate values

### Identifying Duplicates
"""

duplicates=train_clean[train_clean.duplicated()]
num_of_duplicates=duplicates.shape[0]

print(f"The number of duplicate rows: {num_of_duplicates}" )

"""### Handling these duplicates"""

train_clean=train_clean.drop_duplicates()
print(f"The number iof duplicates after removal: {train_clean.duplicated().sum()}")

"""# Handling Outliers"""

print(f"Shape of the cleaned dataset: {train_clean.shape}")

sns.boxplot(x=train_clean['price'])

#sns.boxplot(x=train_clean['column_name'])--> for visualizing
numeric_cols = train_clean.select_dtypes(include=['int64', 'float64']).columns
outlier_counts={}
def outliers(train_clean,cols):
    for col in cols:
        Q1 = train_clean[col].quantile(0.25)
        Q3 = train_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        outliers = train_clean[(train_clean[col] < lower) | (train_clean[col] > upper)]
        outlier_counts[col] = outliers.shape[0]
    return outlier_counts
result=outliers(train_clean,numeric_cols)
print(f"The total number of outliers in columns:{result}")

print(f"Original shape: {train_clean.shape}")

numerical_cols = train_clean.select_dtypes(include=["int64", "float64"]).columns

def remove_outliers(train_clean, cols):
    for col in cols:
        Q1 = train_clean[col].quantile(0.25)
        Q3 = train_clean[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        train_clean = train_clean[(train_clean[col] >= lower_bound) & (train_clean[col] <= upper_bound)]
    return train_clean

train_clean = remove_outliers(train_clean, numerical_cols)

print(f"Shape after outlier removed: {train_clean.shape}")

"""# Data Visualization & Key Insights

## 1. Average Price by Airline
"""

plt.figure(figsize=(10, 5))
sns.barplot(data=train_clean, x='airline', y='price', estimator='mean', palette='Set2')
plt.title('Average Flight Price by Airline')
plt.xlabel('Airline')
plt.ylabel('Average Price')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""This bar plot shows the average ticket price for each airline.

**Insight:**  
Different airlines have varied pricing. Premium airlines like **Vistara or Air India charge more**, while budget airlines like **IndiGo or SpiceJet typically offer lower fares**.

## 2. Duration vs Price
"""

plt.figure(figsize=(10, 6))
sns.scatterplot(data=train_clean, x='duration', y='price', alpha=0.3)
sns.regplot(data=train_clean, x='duration', y='price', scatter=False, color='red')
plt.title('Duration vs Price')
plt.tight_layout()
plt.show()

"""Scatter plot showing the relationship between flight duration and price. A regression line indicates the trend.

**Insight:**  
Thereâ€™s a weak positive correlation â€” longer flights can cost more. However, pricing also depends on other factors like class, route, and stops.

## 3. Price Distribution by Class
"""

plt.figure(figsize=(8, 6))
sns.boxplot(data=train_clean, x='class', y='price')
plt.title('Price Distribution by Class')
plt.tight_layout()
plt.show()

"""A box plot comparing the distribution of ticket prices between Economy and Business class.

**Insight:**  
Business class tickets are significantly more expensive, with a wider price range. This plot also reveals outliers and median prices for both classes.

## 4. Average Price by Number of Stops
"""

plt.figure(figsize=(8, 6))
sns.barplot(data=train_clean, x='stops', y='price', estimator=np.mean)
plt.title('Average Price by Number of Stops')
plt.tight_layout()
plt.show()

"""Bar plot displaying how ticket prices vary with the number of stops (zero, one, etc.).

**Insight:**  
Flights with no stops (direct) are usually more expensive. Adding stops tends to reduce prices due to less convenience.

# Scaling Numerical & Encoding Categorical features
"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split

# Step 1: Input and target
X = train_clean.drop(columns=['price'])
y = train_clean['price']

# Step 2: Custom encode 'stops' and 'class'
stops_map = {'zero': 0, 'one': 1, 'two_or_more': 2}
class_map = {'Economy': 0, 'Business': 1}

X['stops_encoded'] = X['stops'].map(stops_map)
X['class_encoded'] = X['class'].map(class_map)
# Drop original columns
X.drop(columns=['stops', 'class'], inplace=True)

# Step 3: Define feature types
numerical_cols = ['duration', 'days_left']
categorical_cols = ['airline', 'source', 'destination', 'departure', 'arrival']

# Step 4: Transform features
scaler = StandardScaler()
ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')
# Scale numerical columns
X_scaled = pd.DataFrame(scaler.fit_transform(X[numerical_cols]),
                        columns=[col + '_scaled' for col in numerical_cols],
                        index=X.index)

# One-hot encode categorical columns
X_ohe = pd.DataFrame(ohe.fit_transform(X[categorical_cols]),
                     columns=ohe.get_feature_names_out(categorical_cols),
                     index=X.index)

# Keep encoded 'stops' and 'class'
X_others = X.drop(columns=numerical_cols + categorical_cols)  # Contains stops_encoded, class_encoded

# Step 5: Combine all features
X_final = pd.concat([X_scaled, X_ohe, X_others], axis=1)

X_final.dtypes[X_final.dtypes == 'object']

X_final.drop(columns=['flight'], inplace=True)

X_final = X_final.drop(columns=['id'], errors='ignore')

# Step 6: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)

"""# Model Building

## 1. Linear Regression
"""

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

from sklearn.linear_model import LinearRegression
lr=LinearRegression()
lr.fit(X_train,y_train)
prediction = lr.predict(X_test)
lasso_r2 = r2_score(y_test, prediction)
lasso_mae = mean_absolute_error(y_test, prediction)
lasso_rmse = np.sqrt(mean_squared_error(y_test, prediction))

print(f'R2 Score: {lasso_r2}')
print(f'MAE: {lasso_mae}')
print(f'RMSE: {lasso_rmse}')

"""## 2. Ridge Regression"""

from sklearn.linear_model import Ridge
rid_model=Ridge()
rid_model.fit(X_train,y_train)
prediction=rid_model.predict(X_test)
print(f'R2 Score: {r2_score(y_test, prediction)}')
print(f'MAE:{mean_absolute_error(y_test, prediction)}')
print(f'RMSE: {np.sqrt(mean_squared_error(y_test, prediction))}')

"""## 3. Lasso Regression"""

from sklearn.linear_model import Lasso
las_model=Lasso()
las_model.fit(X_train,y_train)
prediction=las_model.predict(X_test)
print(f'R2 Score:{r2_score(y_test, prediction)}')
print(f'MAE: {mean_absolute_error(y_test, prediction)}')
print(f'RMSE: {np.sqrt(mean_squared_error(y_test, prediction))}')

"""## 4. Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor
rf=RandomForestRegressor()
rf.fit(X_train,y_train)
prediction=rf.predict(X_test)
print(f'R2 Score:{r2_score(y_test,prediction)}')
print(f'MAE: {mean_absolute_error(y_test, prediction)}')
print(f'RMSE: {np.sqrt(mean_squared_error(y_test, prediction))}')

"""## 5. Gradient Boosting Regressor"""

from sklearn.ensemble import GradientBoostingRegressor
gb=GradientBoostingRegressor()
gb.fit(X_train,y_train)
prediction=gb.predict(X_test)
print(f'R2 Score:{r2_score(y_test,prediction)}')
print(f'MAE: {mean_absolute_error(y_test, prediction)}')
print(f'RMSE: {np.sqrt(mean_squared_error(y_test, prediction))}')

"""## 6. XG Boost Regressor"""

pip install xgboost

from xgboost import XGBRegressor
model = XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42)
model.fit(X_train,y_train)
prediction=model.predict(X_test)
print(f'R2 Score:{r2_score(y_test,prediction)}')
print(f'MAE: {mean_absolute_error(y_test, prediction)}')
print(f'RMSE: {np.sqrt(mean_squared_error(y_test, prediction))}')

"""## 7. Extra Trees Regressor"""

from sklearn.ensemble import ExtraTreesRegressor
et=ExtraTreesRegressor()
et.fit(X_train,y_train)
prediction=et.predict(X_test)
print(f'R2 Score:{r2_score(y_test,prediction)}')
print(f'MAE: {mean_absolute_error(y_test, prediction)}')
print(f'RMSE: {np.sqrt(mean_squared_error(y_test, prediction))}')

"""## Other Models checked -->LightBGM, CatBoost, AdaBoost and KNeighbours

### LightBGM and CatBoost
"""

# pip install lightgbm catboost
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor

# # Initialize models
# lgb_model = LGBMRegressor(random_state=42)
# cat_model = CatBoostRegressor(verbose=0, random_state=42)  # Set verbose=0 to reduce logs
# lgb_model.fit(X_train, y_train)
# cat_model.fit(X_train, y_train)
# from sklearn.metrics import r2_score, mean_squared_error
# import numpy as np

# # Predict
# lgb_pred = lgb_model.predict(X_test)
# cat_pred = cat_model.predict(X_test)

# # Evaluate
# print("LightGBM RÂ²:", r2_score(y_test, lgb_pred))
# print("CatBoost RÂ²:", r2_score(y_test, cat_pred))
# print("LightGBM RMSE:", np.sqrt(mean_squared_error(y_test, lgb_pred)))
# print("CatBoost RMSE:", np.sqrt(mean_squared_error(y_test, cat_pred)))

"""### AdaBoostRegressor"""

# from sklearn.ensemble import AdaBoostRegressor
# ab=AdaBoostRegressor()
# ab.fit(X_train,y_train)
# prediction=ab.predict(X_test)
# print(r2_score(y_test,prediction))
# print(mean_absolute_error(y_test, prediction))
# print(np.sqrt(mean_squared_error(y_test, prediction)))

"""### KNeighborsRegressor"""

# from sklearn.neighbors import KNeighborsRegressor
# vr=KNeighborsRegressor()
# vr.fit(X_train,y_train)
# prediction=vr.predict(X_test)
# print(r2_score(y_test,prediction))
# print(mean_absolute_error(y_test, prediction))
# print(np.sqrt(mean_squared_error(y_test, prediction)))

"""**It is clear that the best result is obtained by RandomForest, XGBoost, ExtraTrees and CatBoost Regressors. So we are doing Hyperparameter Tuning of these models**

# Hyperparameter Tuning

## 1. Random Forest Regressor
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

rf = RandomForestRegressor(random_state=42)

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10,20, None],
    'min_samples_split': [2, 5],
}

grid_rf = GridSearchCV(rf, param_grid, scoring='r2', cv=3, n_jobs=-1, verbose=2)
grid_rf.fit(X_train, y_train)

print("Best Params:", grid_rf.best_params_)
print("Best RÂ² (CV):", grid_rf.best_score_)
print("Test RÂ²:", grid_rf.best_estimator_.score(X_test, y_test))

"""## 2. Ridge Model"""

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

ridge = Ridge()

param_grid = {
    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]
}

ridge_grid = GridSearchCV(
    estimator=ridge,
    param_grid=param_grid,
    scoring='r2',
    cv=5,
    n_jobs=-1,
    verbose=2
)

ridge_grid.fit(X_train, y_train)

print("ðŸ”§ Best alpha (Ridge):", ridge_grid.best_params_['alpha'])
print("ðŸ“ˆ Best CV RÂ² Score (Ridge):", ridge_grid.best_score_)
print("ðŸ§ª Test RÂ² Score (Ridge):", ridge_grid.best_estimator_.score(X_test, y_test))

"""## 3. Extra Trees Regressor"""

from sklearn.ensemble import ExtraTreesRegressor
from sklearn.model_selection import GridSearchCV

et = ExtraTreesRegressor(random_state=42)

param_grid_et = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'max_features': ['sqrt', 'log2']
}

grid_et = GridSearchCV(
    estimator=et,
    param_grid=param_grid_et,
    scoring='r2',
    cv=3,
    n_jobs=-1,
    verbose=2
)

grid_et.fit(X_train, y_train)

print("âœ… Best Params (ExtraTrees):", grid_et.best_params_)
print("ðŸ“ˆ Best CV RÂ² Score:", grid_et.best_score_)
print("ðŸ§ª Test RÂ² Score:", grid_et.best_estimator_.score(X_test, y_test))

"""## 4. XG Boost Regressor"""

from xgboost import XGBRegressor
from sklearn.model_selection import GridSearchCV



param_grid_xgb = {
    'n_estimators': [100, 200],
    'max_depth': [3, 6, 10],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8, 1.0]
}
xgb = XGBRegressor(random_state=42)
grid_xgb = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid_xgb,
    scoring='r2',
    cv=3,
    n_jobs=-1,
    verbose=2
)

grid_xgb.fit(X_train, y_train)

print("âœ… Best Params (XGBoost):", grid_xgb.best_params_)
print("ðŸ“ˆ Best CV RÂ² Score:", grid_xgb.best_score_)
print("ðŸ§ª Test RÂ² Score:", grid_xgb.best_estimator_.score(X_test, y_test))

"""# Comparison of model performances"""

models = {
    'Random Forest': grid_rf.best_estimator_,
    'Extra Trees': grid_et.best_estimator_,
    'XGBoost': grid_xgb.best_estimator_,
    'Ridge': ridge_grid.best_estimator_,
    'Lasso': las_model,
    'Linear Regression': lr,
}

results = []

for name, model in models.items():
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    results.append((name, r2, mse, rmse))

# Create results DataFrame
import pandas as pd
results_df = pd.DataFrame(results, columns=['Model', 'RÂ² Score', 'MSE', 'RMSE'])
results_df.sort_values(by='RÂ² Score', ascending=False, inplace=True)

print(results_df)

"""## Cleaning of Test Data"""

print(test['days_left'].skew())
print(test['duration'].skew())

print(test['stops'].unique())

test.isnull().sum()

# Make a copy
test_clean = test.copy()

# Drop irrelevant column
test_clean.drop(columns=['flight'], inplace=True)
test_clean['airline'] = test_clean['airline'].fillna(test_clean['airline'].mode()[0])
test_clean['departure'] = test_clean['departure'].fillna(test_clean['departure'].mode()[0])
test_clean['stops'] = test_clean['stops'].fillna(test_clean['stops'].mode()[0])
# Fill missing numerical values
test_clean['duration'] = test_clean['duration'].fillna(train_clean['duration'].median())
test_clean['days_left'] = test_clean['days_left'].fillna(train_clean['days_left'].mean())

# Map categorical values (same as training)
stops_map = {'zero': 0, 'one': 1, 'two_or_more': 2}
class_map = {'Economy': 0, 'Business': 1}

test_clean['stops_encoded'] = test_clean['stops'].map(stops_map)
test_clean['class_encoded'] = test_clean['class'].map(class_map)

# Drop original stops/class
test_clean.drop(columns=['stops', 'class'], inplace=True)

print(test_clean.columns)
print(test_clean[['duration', 'days_left']].head())

# Convert columns to float
test_clean['duration'] = test_clean['duration'].astype(float)
test_clean['days_left'] = test_clean['days_left'].astype(float)
test_clean.columns

# Step 1: Clean raw values (handle bad strings or missing entries)
test_clean['duration'] = pd.to_numeric(test_clean['duration'], errors='coerce')
test_clean['days_left'] = pd.to_numeric(test_clean['days_left'], errors='coerce')

# Step 2: Fill missing values using training stats
test_clean['duration']=test_clean['duration'].fillna(train_clean['duration'].median())
test_clean['days_left']=test_clean['days_left'].fillna(train_clean['days_left'].mean())

# Step 3: Verify they are now float
print(test_clean.dtypes[['duration', 'days_left']])

# Step 4: Print a few values to confirm it's good
print(test_clean[['duration', 'days_left']].head())

X_train.columns

"""## Scaling of Test Dataset"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Step 3: Transform test numerical data
X_scaled_test = pd.DataFrame(
    scaler.transform(test_clean[numerical_cols]),
    columns=[col + '_scaled' for col in numerical_cols],
    index=test_clean.index
)

# Step 4: Transform test categorical data
X_ohe_test = pd.DataFrame(
    ohe.transform(test_clean[categorical_cols]),
    columns=ohe.get_feature_names_out(categorical_cols),
    index=test_clean.index
)

# Step 5: Keep manually encoded columns
X_other_test = test_clean[['stops_encoded', 'class_encoded']]

# Step 6: Combine all into final test feature set
X_final_test = pd.concat([X_scaled_test, X_ohe_test, X_other_test], axis=1)

# Optional: check shape
print("Test set ready:", X_final_test.shape)

X_final_test.columns

X_final_test.isnull().sum()

print(test.shape[0])           # Should be 10000
print(X_final_test.shape[0])   # Should also be 10000

"""# Final Submission"""

rf_pred = grid_rf.best_estimator_.predict(X_final_test)
xgb_pred = grid_xgb.best_estimator_.predict(X_final_test)
y_pred = 0.5 * rf_pred + 0.5 * xgb_pred

submission = pd.DataFrame({
    'id': test['id'],
    'price': y_pred.round(2)  # Optional rounding
})

submission.to_csv('submission.csv', index=False)

# import os

# # Replace with the exact filename you want to delete
# file_to_delete = 'submission.csv'

# # Delete if it exists
# if os.path.exists(file_to_delete):
#     os.remove(file_to_delete)
#     print(f"Deleted '{file_to_delete}'")
# else:
#     print(f"File '{file_to_delete}' not found")