# -*- coding: utf-8 -*-
"""IITM_KA2_23f3001900

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/drishya23f3001900/iitm-ka2-23f3001900.f28498da-5344-4be6-8b99-cb2c9cd54b97.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250905/auto/storage/goog4_request%26X-Goog-Date%3D20250905T093108Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6f31a15e8b04f45bdc9c11920e3fd18da495aabf7acf53169320c4cd238031b4ec5d9fe0ce3bbbb29c23380c9322788b8d16fc7094b3de883ecbb07721f905fc5e5530a935584930df308840727f96527b2905b7abbb5272dae6df9d2b255dc998d6c4dadff9d928bfbce5c7f52042c99431c580e2779592d2f9d053fe477ba0e4fd501d40292aca5c3df9cd26a7bf93c1fb2b984d1cb259e0581982a282e9f9cd841cd648e135d9719a511e0820616e9aea916284d354ee5a05f1a99e52eb6b080fed0f6639728d7f51a9bae0d2fcbe6affdd34293edb8b43e178687f90d9bd29e41a02336f2b14ef6bd7a60687f0c9d769c9b1b46a056f24ae0be7a39c98a6
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

mlp_term_2_2025_kaggle_assignment_2_path = kagglehub.competition_download('mlp-term-2-2025-kaggle-assignment-2')

print('Data source import complete.')

"""This is the final version of my notebook. Forked from my earlier version where I achieved a lower score.

# Problem Statement

#### The objective of this classification task is to predict customer churn for a financial institution. Specifically, the goal is to determine whether a customer will exit (i.e., close their account) or stay with the bank, based on their personal attributes, banking activity, and demographic information.

#### We are provided with a labeled training dataset containing various customer features and their corresponding exit status:

### exit_status = 1 → Customer exited (churned)

### exit_status = 0 → Customer stayed

# Importing Libraries
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

train=pd.read_csv('/kaggle/input/mlp-term-2-2025-kaggle-assignment-2/train.csv')
test=pd.read_csv('/kaggle/input/mlp-term-2-2025-kaggle-assignment-2/test.csv')

raw_test=pd.read_csv('/kaggle/input/mlp-term-2-2025-kaggle-assignment-2/test.csv')

"""# Identifying Data Types"""

train.info()

datatypes=pd.DataFrame({
    'Columns': train.columns,
    'Datatypes':train.dtypes.values
})
datatypes

"""# Descriptive Statistics"""

train.describe().T

"""# Identifying and Handling Missing Values

## Training Dataset
"""

missing=train.isnull().sum()
missing=missing[missing>0]
missing_df=pd.DataFrame({
    'Columns':missing.index,
    'No.of missing values':missing.values
})
missing_df

train.head()

#FOR VISUALIZATION
#sns.histplot(train['credit_score'].dropna(), kde=True)
#sns.histplot(train['acc_balance'].dropna(), kde=True)
#sns.histplot(train['prod_count'].dropna(), kde=True)

num_cols=train.select_dtypes(exclude="object").columns
plt.figure(figsize=(12,8))
sns.heatmap(train[num_cols].corr(),annot=True)
plt.show()

"""#### Missing Value Imputation Strategy
To preserve predictive signal while handling missing data, we applied a combination of:

Grouped Imputation based on relevant segments:

##### credit_score: Imputed using median by (country, age_bin)

##### acc_balance: Median within prod_count groups

##### prod_count: Median by is_active flag

##### country: Imputed using global mode

##### Missingness Indicator Features:
Added binary flags (missing_*) for each column with missing values, allowing the model to learn patterns associated with missingness, which can carry predictive power in real-world behavioral data.

Feature Binning:
Created an age_bin column to discretize age into ordinal buckets to assist group-wise imputation of credit_score.
"""

train['age_bin'] = pd.cut(train['age'], bins=[17, 30, 40, 50, 60, 100], labels=False, include_lowest=True)

#train['age'].unique()
#test['age'].unique()

# 2. Create missing indicators for train dataset
train['missing_credit_score'] = train['credit_score'].isna().astype(int)
train['missing_acc_balance'] = train['acc_balance'].isna().astype(int)
train['missing_prod_count'] = train['prod_count'].isna().astype(int)
train['missing_country'] = train['country'].isna().astype(int)

# 3. Impute credit_score using group median (by country & age_bin)
train['credit_score'] = train.groupby(['country', 'age_bin'])['credit_score'].transform(lambda x: x.fillna(x.median()))

# 4. Impute acc_balance using group median (by prod_count)
train['acc_balance'] = train.groupby(['prod_count'])['acc_balance'].transform(lambda x: x.fillna(x.median()))

# 5. Impute prod_count using group median (by is_active)
train['prod_count'] = train.groupby(['is_active'])['prod_count'].transform(lambda x: x.fillna(x.median()))


# 6. Impute country using mode
train['country'] = train['country'].fillna(train['country'].mode()[0])

"""After group-wise median imputation (based on country, age_bin, or prod_count), some values may still remain missing due to group sparsity or lack of sufficient non-null observations.
As a fallback, we impute these remaining missing values with the overall feature median to ensure no nulls persist in the final training dataset.
"""

train.isnull().sum()

from sklearn.impute import SimpleImputer
median_imputer=SimpleImputer(strategy='median')
train['credit_score']=pd.DataFrame(median_imputer.fit_transform(train[['credit_score']]),index=train.index,columns=['credit_score'])
train['acc_balance']=pd.DataFrame(median_imputer.fit_transform(train[['acc_balance']]),index=train.index,columns=['acc_balance'])

train.isnull().sum()

"""## Testing Dataset"""

# 1. Use same age bins as training
test['age_bin'] = pd.cut(test['age'], bins=[17, 30, 40, 50, 60, 100], labels=False, include_lowest=True)

# 2. Create missing indicators for test dataset
test['missing_credit_score'] = test['credit_score'].isna().astype(int)
test['missing_acc_balance'] = test['acc_balance'].isna().astype(int)
test['missing_prod_count'] = test['prod_count'].isna().astype(int)
test['missing_country'] = test['country'].isna().astype(int)

# For test dataset imputation
def impute_credit_score(row):
    if pd.notnull(row['credit_score']):
        return row['credit_score']
    try:
        return group_medians_credit.loc[(row['country'], row['age_bin'])]
    except KeyError:
        return train['credit_score'].median()  # fallback

def impute_acc_balance(row):
    if pd.notnull(row['acc_balance']):
        return row['acc_balance']
    try:
        return group_medians_acc.loc[row['prod_count']]
    except KeyError:
        return train['acc_balance'].median()

def impute_prod_count(row):
    if pd.notnull(row['prod_count']):
        return row['prod_count']
    try:
        return group_medians_prod.loc[row['is_active']]
    except KeyError:
        return train['prod_count'].median()

# 2. Impute credit_score using (country, age_bin) median from train
group_medians_credit = train.groupby(['country', 'age_bin'])['credit_score'].median()


test['credit_score'] = test.apply(impute_credit_score, axis=1)

# 3. Impute acc_balance using prod_count median from train
group_medians_acc = train.groupby(['prod_count'])['acc_balance'].median()


test['acc_balance'] = test.apply(impute_acc_balance, axis=1)

# 4. Impute prod_count using is_active median from train
group_medians_prod = train.groupby(['is_active'])['prod_count'].median()



test['prod_count'] = test.apply(impute_prod_count, axis=1)

# 5. Impute country using mode from train
test['country'] = test['country'].fillna(train['country'].mode()[0])

test.isnull().sum()

"""# Identifying and Handling Duplicates

## Training Dataset
"""

duplicates=train[train.duplicated()]
num_of_duplicates=duplicates.shape[0]

print(f"The number of duplicate rows: {num_of_duplicates}" )

train=train.drop_duplicates()
print(f"The number of duplicates after removal: {train.duplicated().sum()}")

"""## Testing Dataset"""

duplicates=test[test.duplicated()]
num_of_duplicates=duplicates.shape[0]

print(f"The number of duplicate rows: {num_of_duplicates}" )

test=test.drop_duplicates()

"""# Identifying and Handling Outliers

## Training Dataset
"""

#sns.boxplot(x=train['column_name'])--> for visualizing
# Exclude binary/categorical columns from outlier detection
excluded_cols = ['exit_status', 'has_card'] + [col for col in train.columns if col.startswith('missing') or col == 'age_bin']
numeric_cols = [col for col in train.select_dtypes(include=['int64', 'float64']).columns if col not in excluded_cols]
outlier_counts={}
def outliers(train,cols):
    for col in cols:
        Q1 = train[col].quantile(0.25)
        Q3 = train[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        outliers = train[(train[col] < lower) | (train[col] > upper)]
        if outliers.shape[0]>0:
            outlier_counts[col] = outliers.shape[0]
    return outlier_counts
result=outliers(train,numeric_cols)
print(f"The total number of outliers in columns:{result}")

caps=dict() #to store the capping values that can be used in test dataset
def cap_outliers(train, col):
    Q1 = train[col].quantile(0.25)
    Q3 = train[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    caps[col]=(lower,upper)
    train[col] = train[col].clip(lower=lower, upper=upper)

for col in ['credit_score', 'age', 'prod_count']:
    cap_outliers(train, col)

"""## Testing Dataset"""

#Handling the outliers in the testing dataset according to training dataset
for col, (lower_cap, upper_cap) in caps.items():
    test[col] = test[col].clip(lower=lower_cap, upper=upper_cap)

"""# Visualization and Key Insights"""

sns.countplot(x='age_bin', hue='exit_status', data=train)
plt.title('Exit Status by Age Bin')
plt.xlabel('Age Bin')
plt.ylabel('Count')

# Replace numeric bin labels with readable age ranges
plt.xticks(ticks=[0, 1, 2, 3, 4], labels=['18–30', '31–40', '41–50', '51–60', '60+'])

plt.legend(title='Exit Status', labels=['Stayed', 'Exited'])
plt.tight_layout()
plt.show()

"""### Insight:

Customers aged 31–40 form the largest segment and are the least likely to exit.

Exit rates increase notably in the 41–50 and 51–60 age bins.

Customers aged 60+ show relatively low exit and engagement, possibly due to a smaller sample.

### Takeaway: Age is a strong predictor — especially customers above 40 are more likely to churn.
"""

exit_rate_by_country = train.groupby('country')['exit_status'].mean().sort_values()
sns.barplot(x=exit_rate_by_country.index, y=exit_rate_by_country.values)
plt.xticks(rotation=45)
plt.grid(axis='y')

"""### Insight:

Germany has the highest exit rate, clearly standing out from other countries.

France and Spain have similar and relatively low churn rates.

### Takeaway: Country of residence impacts churn, with Germany showing signs of higher dissatisfaction or disengagement.
"""

exit_rate_by_product = train.groupby('prod_count')['exit_status'].mean()
sns.barplot(x=exit_rate_by_product.index, y=exit_rate_by_product.values)
plt.xlabel("Number of Products")
plt.ylabel("Exit Rate")
plt.grid(axis='y')

"""### Insight:

Customers with 2 products have the lowest churn rate, indicating higher satisfaction or engagement.

Those with 3 or more products show an extremely high churn rate, possibly due to:

Overburdened account handling

Ineffective cross-selling or up-selling

Even 1-product customers show a moderate churn rate.

### Takeaway: Optimal engagement seems to be around 2 products; more isn’t always better — flag high product count customers for retention strategies.
"""

sns.countplot(x='exit_status', data=train)

"""### 4. Class Imbalance (Exit Status)
### Insight:

There’s a clear class imbalance:

~20% customers exited (1)

~80% customers stayed (0)

This imbalance needs to be addressed using methods like:

Resampling (SMOTE/undersampling)

Stratified K-Fold Cross-validation

F1-score/ROC-AUC over accuracy

### Takeaway: Imbalanced classification problem — evaluation and training strategies must account for it to avoid misleading performance metrics.

# Scaling and Encoding
"""

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

"""🎯 stratify=y ensures class balance is maintained when splitting the dataset.
Why it's important:
In classification problems — if your classes are imbalanced (e.g., 80% stayed, 20% exited), a regular train_test_split() might randomly create a test set with mostly one class.


"""

test.info()

"""## Train-Test split"""

X = train.drop("exit_status", axis=1)
y = train["exit_status"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# id and customer_id are unique for every row, hence better to drop
X_train=X_train.drop(['id','customer_id'],axis=1)
X_test=X_test.drop(['id','customer_id'],axis=1)
test=test.drop(['id','customer_id'],axis=1)

"""## Encoding Categorical & Scaling Numerical Features"""

cat_cols=X_train.select_dtypes(include="object").columns
cat_cols

X_train['last_name'].unique() #Many Values, better to drop
X_train['gender'].unique() # Only 2 values, better to use OneHotEncoder
X_train['country'].unique() #Only 3 values, better to use OneHotEncoder

X_train=X_train.drop(['last_name'],axis=1)
X_test=X_test.drop(['last_name'],axis=1)
test=test.drop(['last_name'],axis=1)

from sklearn.preprocessing import OneHotEncoder, StandardScaler

num_cols=X_train.select_dtypes(exclude="object").columns
num_cols
# num_cols=X_train.select_dtypes(exclude="object").columns
# num_cols

ct=ColumnTransformer([
    ('tr1',OneHotEncoder(drop="first",sparse_output=False,handle_unknown='ignore'),['country','gender']),
    ('tr2',StandardScaler(),num_cols)
],remainder="passthrough",verbose_feature_names_out=False).set_output(transform="pandas")
X_train=ct.fit_transform(X_train)
X_test=ct.transform(X_test)
test=ct.transform(test)

"""# Model Building

## Logistic Regression
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

from sklearn.linear_model import LogisticRegression

log_model=LogisticRegression()
log_model.fit(X_train,y_train)
prediction=log_model.predict(X_test)
accuracy = accuracy_score(y_test, prediction)
precision = precision_score(y_test, prediction)
recall = recall_score(y_test, prediction)
f1 = f1_score(y_test, prediction)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, prediction))

"""## K Nearest Neighbors Classifier"""

from sklearn.neighbors import KNeighborsClassifier

knn_model=KNeighborsClassifier()
knn_model.fit(X_train,y_train)
prediction = knn_model.predict(X_test)
accuracy = accuracy_score(y_test, prediction)
precision = precision_score(y_test, prediction)
recall = recall_score(y_test, prediction)
f1 = f1_score(y_test, prediction)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, prediction))

"""## Ada Boost Classifier"""

from sklearn.ensemble import AdaBoostClassifier

adb_model=AdaBoostClassifier()
adb_model.fit(X_train,y_train)
prediction = adb_model.predict(X_test)
accuracy = accuracy_score(y_test, prediction)
precision = precision_score(y_test, prediction)
recall = recall_score(y_test, prediction)
f1 = f1_score(y_test, prediction)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, prediction))

"""## DecisionTree Classifier"""

from sklearn.tree import DecisionTreeClassifier

dt_model=DecisionTreeClassifier()
dt_model.fit(X_train,y_train)
prediction = dt_model.predict(X_test)
accuracy = accuracy_score(y_test, prediction)
precision = precision_score(y_test, prediction)
recall = recall_score(y_test, prediction)
f1 = f1_score(y_test, prediction)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, prediction))

"""## Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

rf_model=RandomForestClassifier()
rf_model.fit(X_train,y_train)
prediction = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, prediction)
precision = precision_score(y_test, prediction)
recall = recall_score(y_test, prediction)
f1 = f1_score(y_test, prediction)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, prediction))

"""## Gradient Boosting Classifier"""

from sklearn.ensemble import GradientBoostingClassifier

gb_model=GradientBoostingClassifier()
gb_model.fit(X_train,y_train)
prediction = gb_model.predict(X_test)
accuracy = accuracy_score(y_test, prediction)
precision = precision_score(y_test, prediction)
recall = recall_score(y_test, prediction)
f1 = f1_score(y_test, prediction)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, prediction))

"""## XGBoost Classifier"""

pip install xgboost

from xgboost import XGBClassifier

xgb_model=XGBClassifier()
xgb_model.fit(X_train,y_train)
prediction = xgb_model.predict(X_test)
accuracy = accuracy_score(y_test, prediction)
precision = precision_score(y_test, prediction)
recall = recall_score(y_test, prediction)
f1 = f1_score(y_test, prediction)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, prediction))

"""## HistGradient Boosting Classifier"""

from sklearn.ensemble import HistGradientBoostingClassifier

hist_model = HistGradientBoostingClassifier(random_state=42)
hist_model.fit(X_train, y_train)
y_pred_hgb = hist_model.predict(X_test)

print("HGB F1 Score:", f1_score(y_test, y_pred_hgb))
print("Precision:", precision_score(y_test, y_pred_hgb))
print("Recall:", recall_score(y_test, y_pred_hgb))
print("Accuracy:", accuracy_score(y_test, y_pred_hgb))

"""### LightBGM"""

!pip install lightgbm --quiet

# Import required libraries
import lightgbm as lgb

# Create the model
lgbm = lgb.LGBMClassifier(random_state=42)

# Train on train set
lgbm.fit(X_train, y_train)

# Predict on test set
y_pred_lgb = lgbm.predict(X_test)

# Evaluate performance
print("LightGBM Accuracy:", accuracy_score(y_test, y_pred_lgb))
print("Precision:", precision_score(y_test, y_pred_lgb))
print("Recall:", recall_score(y_test, y_pred_lgb))
print("F1 Score:", f1_score(y_test, y_pred_lgb))

"""# Hyperparameter Tuning"""

from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

"""## Gradient Boost Classifier"""

params_gb = {
    'n_estimators': [100, 150],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}
gb_model=GradientBoostingClassifier(random_state=42)

search_gb = RandomizedSearchCV(estimator=gb_model, param_distributions=params_gb,scoring='f1', n_iter=10, cv=5, n_jobs=-1, random_state=42)
search_gb.fit(X_train,y_train)


print("Best Params:", search_gb.best_params_)
print("Best F1 Score (CV):", search_gb.best_score_)
y_pred = search_gb.best_estimator_.predict(X_test)
print("Test F1 Score:", f1_score(y_test, y_pred))

"""## XG Boost Classifier"""

params_xgb = {
    'n_estimators': [100, 150],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}
xgb_model=XGBClassifier(random_state=42)

search_xgb = RandomizedSearchCV(estimator=xgb_model, param_distributions=params_xgb, scoring='f1', n_iter=20, cv=5, n_jobs=-1, random_state=42)
search_xgb.fit(X_train,y_train)


print("Best Params:", search_xgb.best_params_)
print("Best F1 Score (CV):", search_xgb.best_score_)
y_pred = search_xgb.best_estimator_.predict(X_test)
print("Test F1 Score:", f1_score(y_test, y_pred))

"""## HistGradientBoostingClassifier"""

from sklearn.model_selection import GridSearchCV

hgb = HistGradientBoostingClassifier(random_state=42)

param_grid = {
    'learning_rate': [0.05, 0.1, 0.2],
    'max_iter': [100, 200],
    'max_depth': [None, 7, 10],
    'l2_regularization': [0.0, 1.0, 5.0],
}

grid = GridSearchCV(
    hgb,
    param_grid,
    scoring='f1',
    cv=5,
    n_jobs=-1,
    verbose=1
)

grid.fit(X_train, y_train)
print("Best F1 score:", grid.best_score_)
print("Best parameters:", grid.best_params_)

best_hgb = grid.best_estimator_

# Predict on test data
y_pred = best_hgb.predict(X_test)

# Print test set scores
print("Test F1 Score:", f1_score(y_test, y_pred))
print("Test Precision:", precision_score(y_test, y_pred))
print("Test Recall:", recall_score(y_test, y_pred))
print("Test Accuracy:", accuracy_score(y_test, y_pred))

"""# Comparison of Model Performance"""

models = {
    'Gradient Boost': search_gb.best_estimator_,
    'XGBoost': search_xgb.best_estimator_,
    'Ada Boost': adb_model,
    'K Nearest Neighbors' : knn_model,
    'HistGradient Boosting' : hist_model, #without tuning gave better results
    'Light BGM': lgbm #without tuning gave better results
}

results = []
for name, model in models.items():
    y_pred = model.predict(X_test)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    results.append((name, f1, precision, recall, accuracy))

results_df = pd.DataFrame(results, columns=['Model', 'F1 Score', 'Precision', 'Recall','Accuracy'])
results_df.sort_values(by='F1 Score', ascending=False, inplace=True)

print(results_df)

from sklearn.metrics import ConfusionMatrixDisplay

for name, model in models.items():
    y_pred = model.predict(X_test)
    print(f"Confusion Matrix for {name}")
    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
    plt.show()

# Example: Zoom into the F1 range
plt.figure(figsize=(10, 6))
sns.barplot(data=results_df, x='Model', y='F1 Score', palette='viridis')
plt.title('Model Comparison - F1 Score')
plt.ylabel('F1 Score')
plt.xlabel('Model')

# Set tighter y-axis limits based on your actual scores
plt.ylim(0.56, 0.63)  # Adjust these values based on your actual score range

plt.xticks(rotation=45)
plt.tight_layout()
plt.grid(axis='y')
plt.show()

"""# Final Submission

### Using Voting Classifier
"""

# from sklearn.ensemble import VotingClassifier

# # VotingClassifier with LightGBM included
# voting_clf = VotingClassifier(
#     estimators=[
#         ('hgb', best_hgb),
#         # ('gb', search_gb.best_estimator_),
#         ('xgb', search_xgb.best_estimator_),
#         ('lgb', lgbm)
#     ],
#     voting='soft',  # Use soft voting for probability-based ensemble
#     n_jobs=-1
# )

# # Fit the ensemble model
# voting_clf.fit(X_train, y_train)

# # Predict on validation/test set
# y_pred = voting_clf.predict(X_test)

# # Evaluate
# print("Voting Classifier F1 Score:", f1_score(y_test, y_pred))
# print("Precision:", precision_score(y_test, y_pred))
# print("Recall:", recall_score(y_test, y_pred))
# print("Accuracy:", accuracy_score(y_test, y_pred))

"""### Stacking different models"""

# from sklearn.ensemble import StackingClassifier
# from sklearn.linear_model import LogisticRegression

# # Define base learners
# base_learners = [
#     ('hgb', best_hgb),
#     ('gb', search_gb.best_estimator_),
#     ('lgb', lgb.LGBMClassifier(random_state=42))
# ]

# # Define meta learner
# meta_learner = LogisticRegression(max_iter=1000)

# # Stacking classifier
# stack_clf = StackingClassifier(
#     estimators=base_learners,
#     final_estimator=meta_learner,
#     passthrough=True,  # allows meta-learner to use original features too
#     n_jobs=-1
# )

# # Train
# stack_clf.fit(X_train, y_train)

# # Predict
# y_pred_stack = stack_clf.predict(X_test)

# # Evaluate
# print("Stacking F1 Score:", f1_score(y_test, y_pred_stack))
# print("Precision:", precision_score(y_test, y_pred_stack))
# print("Recall:", recall_score(y_test, y_pred_stack))
# print("Accuracy:", accuracy_score(y_test, y_pred_stack))

"""### Blending"""

# Predict class probabilities
proba_hgb = hist_model.predict_proba(X_test)
proba_gb = search_gb.best_estimator_.predict_proba(X_test)
proba_lgb = lgbm.predict_proba(X_test)
proba_xgb=search_xgb.best_estimator_.predict_proba(X_test)

# Weighted average of probabilities (you can adjust the weights)
# avg_proba = (0.4 * proba_hgb + 0.3 * proba_gb + 0.3 * proba_xgb)
avg_proba = (
    0.55 * proba_hgb +
    0.45 * proba_lgb
)
#giving the best result till now


# Get final predicted class from the highest probability
y_pred_blend = (avg_proba[:, 1] >= 0.5).astype(int)

# Evaluate
print("Blended F1 Score:", f1_score(y_test, y_pred_blend))
print("Precision:", precision_score(y_test, y_pred_blend))
print("Recall:", recall_score(y_test, y_pred_blend))
print("Accuracy:", accuracy_score(y_test, y_pred_blend))

# Predict probabilities on actual test data (submission set)
proba_hgb_test = hist_model.predict_proba(test)
proba_lgb_test = lgbm.predict_proba(test)

# Weighted average of probabilities
final_avg_proba = (
    0.55 * proba_hgb_test +
    0.45 * proba_lgb_test
)

# Convert probabilities to class labels (threshold 0.5)
final_predictions = (final_avg_proba[:, 1] >= 0.5).astype(int)


submission_df = pd.DataFrame({
    'id': raw_test['id'],
    'exit_status': final_predictions
})

# Export to CSV
submission_df.to_csv("submission.csv", index=False)

print("✅ submission.csv generated successfully.")

# # Predict on the final test set using your voting classifier
# y_final_pred = stack_clf.predict(test)

# # Create submission DataFrame using the original 'id' column
# submission = pd.DataFrame({
#     'id': raw_test['id'],
#     'target': y_final_pred
# })

# # Save to CSV
# submission.to_csv('submission.csv', index=False)

# import os

# # Replace with the exact filename you want to delete
# file_to_delete = 'submission.csv'

# # Delete if it exists
# if os.path.exists(file_to_delete):
#     os.remove(file_to_delete)
#     print(f"Deleted '{file_to_delete}'")
# else:
#     print(f"File '{file_to_delete}' not found")